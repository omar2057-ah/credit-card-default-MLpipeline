# -*- coding: utf-8 -*-
"""Credit-card-default (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V1E1Wh32yq5pO4Pi6hwHhSyDHH63ndLH
"""

import numpy as np                                
import matplotlib.pyplot as plt   
from IPython.display import Image                 
from IPython.display import display               
from sklearn.datasets import dump_svmlight_file   
from time import gmtime, strftime                 
import sys                                        
import math                                       
import json
import boto3
import sagemaker
import pandas as pd

bucket = sagemaker.Session().default_bucket()
prefix = 'sagemaker/creditcard-def'
role = sagemaker.get_execution_role()

print('Bucket:\n{}'.format(bucket))

df = pd.read_excel('default of credit card clients.xls', header = 1)
df

df.head(10)

print('The shape of the dataset is:', df.shape)
df.info()

df.isnull().sum()

df_no_miss = df.dropna()
df_no_miss.shape

df_no_miss.describe()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

df_no_miss['default payment next month'].value_counts().plot.hist(bins=70)
plt.show()

df_no_miss['default payment next month'].plot.hist(bins=10)
plt.show()

df_no_miss.boxplot(["AGE"])

print("Number of samples before: {}".format(df_no_miss.shape[0]))
# Save the quartiles
AGE_25 = np.percentile(df_no_miss['AGE'], 25)
AGE_50 = np.percentile(df_no_miss['AGE'], 50)
AGE_75 = np.percentile(df_no_miss['AGE'], 75)

# Calculate the thresholds
IQR_AGE = AGE_75 - AGE_25
Lower_Limit = AGE_50 - IQR_AGE * 1.5
Upper_Limit = AGE_50 + IQR_AGE * 1.5

# Remove the outliers
df = df_no_miss.loc[(df_no_miss['AGE'] > Lower_Limit) &
                              (df_no_miss['AGE'] < Upper_Limit)]
df.boxplot(['AGE'])
print("Number of samples after: {}".format(df.shape[0]))

print("Number of samples before: {}".format(df.shape[0]))
df.drop_duplicates(inplace=True)
print("Number of samples before: {}".format(df.shape[0]))

df = pd.get_dummies(df)

df.head(10)

print('The shape of the dataset is:', df.shape)
df.info()

import matplotlib.pyplot as plt
import seaborn as sns

f, ax = plt.subplots(figsize=(32, 26))
corr = df.corr()
mp = sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax, annot = True)
mp.set_title(label='dataset correlation', fontsize=20)

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(df, test_size=0.2, shuffle=True, random_state=23)
val_data, test_data = train_test_split(test_data, test_size=0.2, shuffle=True, random_state=23)

from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

#pipeline = Pipeline([ ('dt', AdaBoostClassifier()) ])
#pipeline = Pipeline([ ('dt',SVC(kernel="linear", C=0.025)) ])
#pipeline = Pipeline([ ('dt',RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)) ])
pipeline = Pipeline([ ('dt', GradientBoostingClassifier(max_depth=10, random_state=0)) ])
from sklearn import set_config
set_config()
pipeline

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sb
# %matplotlib inline
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

X_train = train_data
X_train = X_train.drop('default payment next month', axis=1)
y_train = train_data['default payment next month']

pipeline.fit(X_train, y_train)

# Use the fitted pipeline to make predictions on the train dataset
train_predictions = pipeline.predict(X_train)

#this is for binary classification
print(confusion_matrix(y_train, train_predictions))
print(classification_report(y_train, train_predictions))
print("Accuracy (training):", accuracy_score(y_train, train_predictions))


# Get test data to test the pipeline
X_test = test_data.drop('default payment next month', axis=1)
y_test = test_data['default payment next month']

# Use the fitted pipeline to make predictions on the test dataset
test_predictions = pipeline.predict(X_test)
print(confusion_matrix(y_test, test_predictions))
print(classification_report(y_test, test_predictions))
print("Accuracy (test):", accuracy_score(y_test, test_predictions))

